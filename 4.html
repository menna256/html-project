
<html>
<head>
<title>history of artifical intelligence</title>
</head>
<body dir="rt1" background="image/19.JPG">
<a href="1.html">main page</a></td>
 &emsp;
  &emsp;
 <a href="2.html"> Applications of artificial intelligence</a></td>
  &emsp;

 <a href="3.html">Areas of application of artificial intelligence</a></td>
  &emsp;

 <a href="4.html">history of artifical intelligence</a></td>
  &emsp;

<h1><center><font color ="black" >history of artifical intelligence</center></h1>
<p><font color ="black" > .<font color ="white" >Thought-capable artificial beings appeared as storytelling devices in antiquity,[37] and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R.[38] These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.[32]  </p>
<p><font color ="black" > .<font color ="white" >The study of mechanical or "formal" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church–Turing thesis.[39] Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. Turing proposed changing the question from whether a machine was intelligent, to "whether or not it is possible for machinery to show intelligent behaviour"  </p>
<ul>
<li><font color ="black" > in 1934The first work that is  generally recognized as AI was McCullouch and Pitts'formal design for Turing-complete "artificial neurons".[41</li>
<li><font color ="black" > in 1956The field of AI research was born at a workshop at Dartmouth College </li>
<li><font color ="black" >in 1954computers were learning checkers strategies </li>
<li><font color ="black" >In the early 1980s, AI research was revived by the commercial success of expert systems,[52] a form of AI program that simulated the knowledge and analytical skills of human experts</li>
<li><font color ="black" >By 1985, the market for AI had reached over a billion dollars.</li>
<li><font color ="black" >in the 1980sThe development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) transistor technology, enabled the development of practical artificial neural network (ANN) technology </li>
<li><font color ="black" >In the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas</li>
<li><font color ="black" > 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google</li>
<li><font color ="black" >By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining commonsense understanding of the contents of the benchmarks</li>

<center>

<img src="image/history.png" width="400" height="400" border="3">
<img src="image/history2.jpg" width="400" height="400" border="3">

</body>
</html>
